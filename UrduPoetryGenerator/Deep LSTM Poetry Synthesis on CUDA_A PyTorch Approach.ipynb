{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Check for CUDA and set device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 12876\n",
      "Maximum sequence length: 431\n"
     ]
    }
   ],
   "source": [
    "# Path to the CSV file containing your poems\n",
    "csv_path = r\"C:\\Users\\Lenovo\\Downloads\\datasetttttt\\poems_dataset.csv\"\n",
    "data = pd.read_csv(csv_path)\n",
    "\n",
    "# Expecting a column named 'poem'\n",
    "if \"poem\" not in data.columns:\n",
    "    raise ValueError(\"CSV file must contain a 'poem' column.\")\n",
    "\n",
    "poems = data[\"poem\"].tolist()\n",
    "\n",
    "# Build vocabulary using whitespace-based tokenization.\n",
    "# You may substitute this with a more robust tokenizer if desired.\n",
    "all_words = []\n",
    "for poem in poems:\n",
    "    all_words.extend(poem.split())\n",
    "\n",
    "# Get unique words and sort them (for reproducibility)\n",
    "vocab = sorted(set(all_words))\n",
    "# Create mappings: reserve index 0 for padding.\n",
    "word_to_idx = {word: idx + 1 for idx, word in enumerate(vocab)}\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "vocab_size = len(word_to_idx) + 1  # +1 for the padding index\n",
    "\n",
    "print(\"Vocabulary size:\", vocab_size)\n",
    "\n",
    "# Create training sequences:\n",
    "# For each poem, generate sequences where the first n tokens are input\n",
    "# and the (n+1)th token is the target.\n",
    "sequences = []\n",
    "for poem in poems:\n",
    "    token_list = [word_to_idx[word] for word in poem.split()]\n",
    "    # Skip poems with fewer than 2 words\n",
    "    if len(token_list) < 2:\n",
    "        continue\n",
    "    for i in range(1, len(token_list)):\n",
    "        # Create an n-gram sequence: tokens[0:i+1]\n",
    "        seq = token_list[: i + 1]\n",
    "        sequences.append(torch.tensor(seq, dtype=torch.long))\n",
    "\n",
    "# Find maximum sequence length\n",
    "max_seq_len = max([len(seq) for seq in sequences])\n",
    "print(\"Maximum sequence length:\", max_seq_len)\n",
    "\n",
    "# Pad all sequences to the same length (pre-padding with 0, our pad index)\n",
    "padded_sequences = []\n",
    "for seq in sequences:\n",
    "    pad_len = max_seq_len - len(seq)\n",
    "    # Create padded sequence: a tensor of zeros (pad) then the sequence tokens\n",
    "    padded_seq = torch.cat((torch.zeros(pad_len, dtype=torch.long), seq))\n",
    "    padded_sequences.append(padded_seq)\n",
    "\n",
    "# Stack into a single tensor of shape (num_sequences, max_seq_len)\n",
    "padded_sequences = torch.stack(padded_sequences)\n",
    "\n",
    "# For each sequence, the input is all tokens except the last;\n",
    "# the target is the last token.\n",
    "inputs = padded_sequences[:, :-1]   # shape: (num_samples, max_seq_len-1)\n",
    "targets = padded_sequences[:, -1]   # shape: (num_samples)\n",
    "\n",
    "# Create a TensorDataset and DataLoader\n",
    "dataset = TensorDataset(inputs, targets)\n",
    "batch_size = 64\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Build the LSTM Model in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len)\n",
    "        embedded = self.embedding(x)       # (batch, seq_len, embed_dim)\n",
    "        lstm_out, (hn, cn) = self.lstm(embedded)\n",
    "        # Use the output at the last time step for prediction\n",
    "        last_output = lstm_out[:, -1, :]   # (batch, hidden_dim)\n",
    "        logits = self.fc(last_output)      # (batch, vocab_size)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "embed_dim = 100\n",
    "hidden_dim = 150\n",
    "num_epochs = 50       # Adjust as needed\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = LSTMModel(vocab_size, embed_dim, hidden_dim).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch [1/50], Loss: 7.0961\n",
      "Epoch [2/50], Loss: 6.3320\n",
      "Epoch [3/50], Loss: 5.7618\n",
      "Epoch [4/50], Loss: 5.2068\n",
      "Epoch [5/50], Loss: 4.6884\n",
      "Epoch [6/50], Loss: 4.2131\n",
      "Epoch [7/50], Loss: 3.7844\n",
      "Epoch [8/50], Loss: 3.4087\n",
      "Epoch [9/50], Loss: 3.0799\n",
      "Epoch [10/50], Loss: 2.7921\n",
      "Epoch [11/50], Loss: 2.7066\n",
      "Epoch [12/50], Loss: 2.5550\n",
      "Epoch [13/50], Loss: 2.2919\n",
      "Epoch [14/50], Loss: 2.0995\n",
      "Epoch [15/50], Loss: 1.9380\n",
      "Epoch [16/50], Loss: 1.7979\n",
      "Epoch [17/50], Loss: 1.6864\n",
      "Epoch [18/50], Loss: 1.6284\n",
      "Epoch [19/50], Loss: 1.5093\n",
      "Epoch [20/50], Loss: 1.3836\n",
      "Epoch [21/50], Loss: 1.2825\n",
      "Epoch [22/50], Loss: 1.1926\n",
      "Epoch [23/50], Loss: 1.1064\n",
      "Epoch [24/50], Loss: 1.0272\n",
      "Epoch [25/50], Loss: 0.9513\n",
      "Epoch [26/50], Loss: 0.8790\n",
      "Epoch [27/50], Loss: 0.8122\n",
      "Epoch [28/50], Loss: 0.7483\n",
      "Epoch [29/50], Loss: 0.6858\n",
      "Epoch [30/50], Loss: 0.6296\n",
      "Epoch [31/50], Loss: 0.5758\n",
      "Epoch [32/50], Loss: 0.5250\n",
      "Epoch [33/50], Loss: 0.4805\n",
      "Epoch [34/50], Loss: 0.4389\n",
      "Epoch [35/50], Loss: 0.4017\n",
      "Epoch [36/50], Loss: 0.3654\n",
      "Epoch [37/50], Loss: 0.3346\n",
      "Epoch [38/50], Loss: 0.3066\n",
      "Epoch [39/50], Loss: 0.2798\n",
      "Epoch [40/50], Loss: 0.2588\n",
      "Epoch [41/50], Loss: 0.2379\n",
      "Epoch [42/50], Loss: 0.2202\n",
      "Epoch [43/50], Loss: 0.2016\n",
      "Epoch [44/50], Loss: 0.1897\n",
      "Epoch [45/50], Loss: 0.1752\n",
      "Epoch [46/50], Loss: 0.1645\n",
      "Epoch [47/50], Loss: 0.1522\n",
      "Epoch [48/50], Loss: 0.1438\n",
      "Epoch [49/50], Loss: 0.1367\n",
      "Epoch [50/50], Loss: 0.1248\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training...\")\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    for batch_inputs, batch_targets in dataloader:\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_targets = batch_targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_inputs)  # (batch_size, vocab_size)\n",
    "        loss = criterion(outputs, batch_targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    avg_loss = epoch_loss / len(dataloader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Text Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_poem(seed_text, next_words=50):\n",
    "    \"\"\"\n",
    "    Generates additional words to complete a poem given a seed text.\n",
    "    :param seed_text: String containing the seed line of poetry.\n",
    "    :param next_words: Number of words to generate.\n",
    "    :return: Completed poem as a string.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    generated_text = seed_text\n",
    "    \n",
    "    for _ in range(next_words):\n",
    "        # Tokenize the current text using the same method as training.\n",
    "        token_list = [word_to_idx.get(word, 0) for word in generated_text.split()]\n",
    "        # Keep only the last (max_seq_len - 1) tokens (if longer, take the tail).\n",
    "        token_list = token_list[-(max_seq_len - 1):]\n",
    "        # Pre-pad the sequence to length (max_seq_len - 1)\n",
    "        pad_len = (max_seq_len - 1) - len(token_list)\n",
    "        token_list = [0] * pad_len + token_list\n",
    "        input_seq = torch.tensor(token_list, dtype=torch.long).unsqueeze(0).to(device)  # shape: (1, max_seq_len-1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(input_seq)  # shape: (1, vocab_size)\n",
    "            predicted_idx = torch.argmax(output, dim=1).item()\n",
    "        \n",
    "        # If predicted index is 0 (unlikely, but reserved for padding), break.\n",
    "        if predicted_idx == 0:\n",
    "            break\n",
    "        \n",
    "        predicted_word = idx_to_word.get(predicted_idx, \"\")\n",
    "        generated_text += \" \" + predicted_word\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Generate a Poem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed Poem:\n",
      "\n",
      "koī mai de yā na de ham riñd-e-be-parvā haiñ aap sāqiyā apnī baġhal meñ shīsha-e-sahbā haiñ aap ġhāfil o hoshyār vo timsāl-e-yak-ā.īna haiñ varta-e-hairat meñ nādāñ aap haiñ daanā haiñ aap kyuuñ rahe merī duā minnat-kash-e-bāl-e-malak nāla-e-mastāna mere āsmāñ-paimā haiñ aap hai ta.ajjub ḳhizr ko aur āb-e-haivāñ kī talab aur phir uzlat-guzīn-e-dāman-e-sahrā haiñ aap manzil-e-tūl-e-amal darpesh aur mohlat hai kam\n"
     ]
    }
   ],
   "source": [
    "seed_line = \"koī mai de yā na de ham riñd-e-be-parvā haiñ aap\"\n",
    "completed_poem = generate_poem(seed_line, next_words=50)\n",
    "print(\"\\nCompleted Poem:\\n\")\n",
    "print(completed_poem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed Poem:\n",
      "\n",
      "rukhsat ke baad bhi, unka ehsaas dil mein qaid hai pahle bhī ek din rahe aur hī kuchh thā gar na thā gar magar na ho kahīñ na hotā to hotā hai na vo jo aaj to vo nahīñ hai ki motī piro.e haiñ par hameñ na vo log hazār ḳhudā kisī ko na jaane jo kisī ko vo ḳhudā\n"
     ]
    }
   ],
   "source": [
    "seed_line = \"rukhsat ke baad bhi, unka ehsaas dil mein qaid hai\"\n",
    "completed_poem = generate_poem(seed_line, next_words=50)\n",
    "print(\"\\nCompleted Poem:\\n\")\n",
    "print(completed_poem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed Poem:\n",
      "\n",
      "dil ke virane mein, phir bhi ishq ka noor chamakta hai apne hī pe lahū 'jālib' qissa to bahut ro.e haiñ dil kyā hotā maiñ un kī gāliyoñ kā ho jaa.e us kāfir ko mil hī na ho ham se ai 'akbar' ḳhud ko bhī jaañ se ma.alūm huā thā vo zindagī bahut der se rahī dostī hai ki log tarah\n"
     ]
    }
   ],
   "source": [
    "seed_line = \"dil ke virane mein, phir bhi ishq ka noor chamakta hai\"\n",
    "completed_poem = generate_poem(seed_line, next_words=50)\n",
    "print(\"\\nCompleted Poem:\\n\")\n",
    "print(completed_poem)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
